{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9b7164-9303-466d-818d-e150aa8de7b5",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4bc3a-5517-4f59-bb5c-b62524d3650a",
   "metadata": {},
   "source": [
    "**Data collection** is the process of gathering data from different sources for analysis and processing. \\In the context of machine learning and natural language processing, collecting relevant and diverse data is crucial for training accurate and robust models.\\In this code snippet, we collect text data from Twitter for three different languages (Darija, French, and English) by searching specific queries for each language and saving the collected tweets into a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13a763-7101-4cd7-b2e5-bbae2a07e08f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.  Install `snscrape` library using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4968135-f5e8-49f2-95c7-346d0c9746a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.6.2.20230320)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from snscrape) (3.9.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from snscrape) (4.9.2)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from snscrape) (2.28.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from snscrape) (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4->snscrape) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests[socks]->snscrape) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests[socks]->snscrape) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests[socks]->snscrape) (1.26.14)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\moham\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf3212-2859-4682-961c-f90f9b4831a4",
   "metadata": {},
   "source": [
    "### 2.  Import required libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c9e2e8-9b85-40aa-ad63-c9f19185b173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb228be-3ebb-4a91-a4b8-b1e519f9936d",
   "metadata": {},
   "source": [
    "### 3.  Define a function to search for tweets using specific queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3c26d5-80f2-4b92-bd5d-f287a9ee5cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_by_query(query):     \n",
    "    return sntwitter.TwitterSearchScraper(query).get_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c44efb-4d5b-42a4-a007-4ce501954666",
   "metadata": {},
   "source": [
    "### 4.  Define a function to collect tweets for a given language and list of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad6bb2b-7678-4f3b-967b-fc6acbe7a4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tweets(maxTweets, language, queries):     \n",
    "    tweets = []     \n",
    "    for query in queries:         \n",
    "        for i, tweet in enumerate(search_by_query(query)):             \n",
    "            tweets.append({'language': language, 'content': tweet.rawContent})             \n",
    "            if i >= maxTweets:                 \n",
    "                break     \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8134270-cecf-4c77-a0eb-9f4ceec45b49",
   "metadata": {},
   "source": [
    "### 5.  Set maximum number of tweets to be collected and define the queries for each language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4933c347-078b-4094-a144-53cb4865856e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxTweets = 3000 \n",
    "queries = {'darija': ['montakhab', 'darija'],\n",
    "           'french': ['amour', 'cin√©ma'],\n",
    "           'english': ['technology', 'quotes', ] }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10530d0-2241-478a-b972-f1c0bcc9615e",
   "metadata": {},
   "source": [
    "### 6.  Collect tweets for each language and append them to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a8338-128d-49cc-ab6d-d3b0ab7b0058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = [] \n",
    "for language, language_queries in queries.items():     \n",
    "    tweets += get_tweets(maxTweets, language, language_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47196d-deca-4e6d-8e8c-c3bb40c1496e",
   "metadata": {},
   "source": [
    "### 7.  Print number of collected tweets and time taken for scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0bcf8-84f5-45b3-a85d-a407cfc443d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Collected {len(tweets)} tweets in {end - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42accd7-852f-4868-b10b-90e6e60e266d",
   "metadata": {},
   "source": [
    "### 8.  Transform the tweets into a Pandas dataframe and save it to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34ba99-154a-4416-b2dd-bc6544d1f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets) df.to_csv('data.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
