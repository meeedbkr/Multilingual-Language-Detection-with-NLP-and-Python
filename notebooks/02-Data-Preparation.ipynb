{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0cd446-32cf-4e58-820b-479fe3112d17",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 02-Data-Preparation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ef069-d403-4e29-96ea-e22f40618f7e",
   "metadata": {},
   "source": [
    "Data preparation refers to the process of cleaning and transforming raw data into a format suitable for analysis.\\\n",
    "This involves several steps, including data cleaning, data integration, data transformation, and data reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a9eb3-89f8-4108-9d6f-abd0b46cb0cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bedbecb-1e49-4290-8b95-83006b7716bf",
   "metadata": {},
   "source": [
    "The first few lines of code import necessary libraries like re, string, nltk, pandas, and SnowballStemmer.\\\n",
    "Additionally, nltk.download() function is used to download necessary data from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4141035a-381c-474c-840f-494404f21891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbe2fa-dfbf-40c4-8411-f5bdbfb1de86",
   "metadata": {},
   "source": [
    "### 2. Define function to preprocess and clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f7f3f3-fad6-42f4-9fd5-0f4afd315d8e",
   "metadata": {},
   "source": [
    "The next step defines a function called `preprocess_text()`.\\\n",
    "This function takes in a text input and performs various cleaning and preprocessing tasks on it. It removes URLs, mentions, and hashtags, removes non-alphabetic characters and numbers, converts text to lowercase, tokenizes text into words, removes stopwords, removes punctuation, stems words, and finally, joins the tokens back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "580a5425-6841-4256-8d13-98fce4155b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Check if the input is a string or bytes object, if not, return an empty string\n",
    "    if not isinstance(text, (str, bytes)):\n",
    "        return ''\n",
    "    # Remove URLs, mentions, and hashtags from the text using regular expressions\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    # Remove non-alphabetic characters and numbers from the text using regular expressions\n",
    "    text = re.findall(r'\\b(?!\\d+\\b)[a-zA-Z0-9]+\\b', text)\n",
    "    # Convert text to lowercase\n",
    "    text = ' '.join(text).lower().strip()\n",
    "    # Tokenize the text into words using NLTK's word_tokenize() function\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords using NLTK's stopwords.words('english') function\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # Remove stopwords using NLTK's stopwords.words('english') function\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('french')]\n",
    "    # Remove punctuation using the string.punctuation constant\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Stem words using NLTK's SnowballStemmer('english') function\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Join the stemmed tokens back into text\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5792ab9-8596-49cc-8c29-552242c2303a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `preprocess_text()` function takes a single argument `text`, which is the input text that needs to be preprocessed.\\\n",
    "The function first checks whether the input is a string or bytes object using the `isinstance()` function. If it is not,\n",
    "the function returns an empty string.\n",
    "\n",
    "Next, the function removes URLs, mentions, and hashtags from the text using regular expressions. This is done using the `re.sub()` function,\n",
    "which replaces any string that matches the regular expression with an empty string.\n",
    "\n",
    "After that, the function removes any non-alphabetic characters and numbers from the text using regular expressions. This is done using the `re.findall()` function, which returns a list of all the strings that match the regular expression.\n",
    "\n",
    "The function then converts the text to lowercase and tokenizes it into words using NLTK's `word_tokenize()` function. It removes stopwords using NLTK's `stopwords.words('english')` function, removes punctuation using the string.punctuation constant, and stems the words using NLTK's `SnowballStemmer('english')` function.\n",
    "\n",
    "Finally, the function joins the stemmed tokens back into text and returns it.\n",
    "\n",
    "Overall, the `preprocess_text()` function performs several common text preprocessing steps, such as removing URLs, mentions, and hashtags, removing non-alphabetic characters and numbers, tokenizing the text into words, removing stopwords and punctuation, and stemming the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634bf05-05ad-4b3f-9adc-196ec9c03173",
   "metadata": {},
   "source": [
    "### 3. Define function to preprocess and clean all collected tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab124f6f-ad3a-43f9-8ca3-f5e25534a17f",
   "metadata": {},
   "source": [
    "The next step defines a function called `preprocess_tweets()`.\\\n",
    "This function takes in a list of tweets and preprocesses each tweet using the `preprocess_text()` function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a5eb69-a09c-46ac-955c-665dd461bd10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    for tweet in tweets:\n",
    "        tweet['content'] = preprocess_text(tweet['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d65ecc-89a4-4e4f-bd25-f44f2d3d1c87",
   "metadata": {},
   "source": [
    "### 4. Read raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac057fff-f2b6-4d8a-b6a1-8403c65e9e18",
   "metadata": {},
   "source": [
    "The next step reads the raw data from a CSV file using the `read_csv()` function of the pandas library. The raw data is stored in a dataframe called `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508ac31e-2cff-4011-9b12-2dce9de14970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df =  pd.read_csv('../data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ffec8-5112-4934-aca2-56b740f837f6",
   "metadata": {},
   "source": [
    "### 5. Clean the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c838ac-3dc8-4f5f-a34b-2054f29ad4cd",
   "metadata": {},
   "source": [
    "The next step applies the `preprocess_text()` function to each row of the `'content'` column of the dataframe using the `apply()` function.\\\n",
    "This cleans and preprocesses the text data in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b421d-a1ca-4a62-bd62-81b7b0bd9ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d5e16-cb7e-4ef2-9d05-1c0ad59adcbd",
   "metadata": {},
   "source": [
    "### 6. Drop duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292062b-de1a-4f86-b59c-d9d689ed5e00",
   "metadata": {},
   "source": [
    "The next step drops duplicate rows from the dataframe based on the 'content' column using the `drop_duplicates()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b9c845-f57c-418b-8417-c3fea5547c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['content'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca41bb0-b6ba-4c32-be7e-69dced875319",
   "metadata": {},
   "source": [
    "### 7. Drop empty content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8dd55-a868-4296-9990-b339b50738bc",
   "metadata": {},
   "source": [
    "The next step drops rows from the dataframe that have empty 'content' using the `drop()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c912f-8a8e-41d3-8d2c-b708c64232e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=df[df['content'] == ''].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d113c9c-85c2-46dd-a34a-f31c6f73f14c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8. Count the number of sentences after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda23beb-1925-486f-bca4-9233982260d7",
   "metadata": {},
   "source": [
    "the next step counts the number of sentences after cleaning, for each language. This can be useful to get an idea of the distribution of the data by language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483f4ba1-b863-407a-b5ae-bc967a31564b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Count the number of sentences after cleaning, for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e920f-38e1-41c3-80fc-bc3d4dea06fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f54328-c2f2-49c1-871e-c08d0b1770d7",
   "metadata": {},
   "source": [
    "The `value_counts()` method returns a series containing counts of unique values in the 'language' column of the cleaned data frame. This will give us a count of the number of sentences in each language after cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8c130-c0d8-4e37-8099-474bb013fd60",
   "metadata": {},
   "source": [
    "### 9. Save clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d9fab-a211-49f4-9b05-d01d8b8562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/clean_data.csv1',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968346df-4646-4d60-a648-73513d06c913",
   "metadata": {},
   "source": [
    "The `to_csv()` method saves the cleaned data frame to a new CSV file.\\\n",
    "The `encoding='utf-8'` parameter specifies that the file should be saved using the UTF-8 encoding, which can handle all possible characters in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d3f51484-2f91-46e6-99fd-429e7fcc7b23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define function to preprocess and clean text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, (str, bytes)):\n",
    "        return ''\n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    # Remove non-alphabetic characters and numbers\n",
    "    text = re.findall(r'\\b(?!\\d+\\b)[a-zA-Z0-9]+\\b', text)\n",
    "    # Convert text to lowercase\n",
    "    text = ' '.join(text).lower().strip()\n",
    "    # Tokenize text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Stem words\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Join tokens back into text\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Define function to preprocess and clean all collected tweets\n",
    "def preprocess_tweets(tweets):\n",
    "    for tweet in tweets:\n",
    "        tweet['content'] = preprocess_text(tweet['content'])\n",
    "        \n",
    "#read raw data\n",
    "df =  pd.read_csv('../data/raw_data.csv')\n",
    "# clean the dataframe\n",
    "df['content'] = df['content'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57a77599-2c02-40d2-b085-3d84ee7fae46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df.drop_duplicates(subset=['content'],inplace=True)\n",
    "# drop empty content\n",
    "df.drop(index=df[df['content'] == ''].index, inplace=True)\n",
    "# counting the number of sentences after cleaning\n",
    "df['language'].value_counts()\n",
    "# save clean data\n",
    "df.to_csv('../data/clean_data.csv',encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
